{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdminMas7er/JBG040-DC1-Group-14/blob/main/Final_notebook_sprint_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I62TXjQZpDdO"
      },
      "source": [
        "#Introduction:\n",
        "In this template, methods are provided to get you started on the task at hand (please see project description). Please implement your solution in the code cells marked with **TODO**. Most of the other code cells are hidden, feel free to explore and change these. These cells implement a basic pipeline for training your model but you may want to explore more complex procedures. **Make sure you run all cells before trying to implement your own solution!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_jdXV_Cj0fY"
      },
      "source": [
        "#Imports and definitions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huq9l3CmzL3L"
      },
      "source": [
        "#@title\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import copy\n",
        "import numpy as np\n",
        "import requests\n",
        "import io\n",
        "from torch.utils.data import TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from scipy.ndimage import rotate\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from scipy.special import softmax\n",
        "import math\n",
        "\n",
        "class BatchSampler():\n",
        "  \"\"\"\n",
        "  Implements an iterable which given a torch dataset and a batch_size\n",
        "  will produce batches of data of that given size. The batches are\n",
        "  returned as tuples in the form (images, labels).\n",
        "  Can produce balanced batches, where each batch will have an equal \n",
        "  amount of samples from each class in the dataset. If your dataset is heavily\n",
        "  imbalanced, this might mean throwing away a lot of samples from \n",
        "  over-represented classes!\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, dataset, balanced=False):\n",
        "    self.batch_size = batch_size\n",
        "    self.dataset = dataset\n",
        "    self.balanced = balanced\n",
        "    if self.balanced:\n",
        "      # Counting the ocurrence of the class labels:\n",
        "      unique, counts = np.unique(self.dataset.targets, return_counts=True) \n",
        "      indexes = []\n",
        "      # Sampling an equal amount from each class:\n",
        "      for i in range(len(unique)):\n",
        "        indexes.append(np.random.choice(np.where(self.dataset.targets == i)[0], size=counts.min(), replace=False))\n",
        "      # Setting the indexes we will sample from later:\n",
        "      self.indexes = np.concatenate(indexes)\n",
        "    else:\n",
        "      # Setting the indexes we will sample from later (all indexes):\n",
        "      self.indexes = [i for i in range(len(dataset))]\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.indexes) // self.batch_size) + 1\n",
        "  \n",
        "  def shuffle(self):\n",
        "    # We do not need to shuffle if we use the balanced sampling method.\n",
        "    # Shuffling is already done when making the balanced samples.\n",
        "    if not self.balanced:\n",
        "      random.shuffle(self.indexes)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    remaining = False\n",
        "    self.shuffle()\n",
        "    # Go over the datset in steps of 'self.batch_size':\n",
        "    for i in range(0, len(self.indexes), self.batch_size):\n",
        "        imgs, labels = [], []\n",
        "        # If our current batch is larger than the remaining data, we quit:\n",
        "        if i + self.batch_size > len(self.indexes):\n",
        "          remaining = True\n",
        "          break\n",
        "        # If not, we yield a complete batch:\n",
        "        else:\n",
        "          # Getting a list of samples from the dataset, given the indexes we defined:\n",
        "          X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, i + self.batch_size)]\n",
        "          Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, i + self.batch_size)]\n",
        "          # Stacking all the samples and returning the target labels as a tensor:\n",
        "          yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "    # If there is still data left that was not a full batch:\n",
        "    if remaining:\n",
        "      # Return the last batch (smaller than batch_size):\n",
        "      X_batch = [self.dataset[self.indexes[k]][0] for k in range(i, len(self.indexes))]\n",
        "      Y_batch = [self.dataset[self.indexes[k]][1] for k in range(i, len(self.indexes))]\n",
        "      yield torch.stack(X_batch).float(), torch.tensor(Y_batch).long()\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Creates a DataSet from numpy arrays while keeping the data \n",
        "  in the more efficient numpy arrays for as long as possible and only\n",
        "  converting to torchtensors when needed (torch tensors are the objects used\n",
        "  to pass the data through the neural network and apply weights).\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, x, y, transform=None, target_transform=None):\n",
        "    self.targets = y\n",
        "    self.imgs = x\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = torch.from_numpy(self.imgs[idx] / 255).float()\n",
        "    label = self.targets[idx]\n",
        "    return image, label\n",
        "\n",
        "def load_numpy_arr_from_url(url):\n",
        "    \"\"\"\n",
        "    Loads a numpy array from surfdrive. \n",
        "    \n",
        "    Input:\n",
        "    url: Download link of dataset \n",
        "    \n",
        "    Outputs:\n",
        "    dataset: numpy array with input features or labels\n",
        "    \"\"\"\n",
        "    \n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    return np.load(io.BytesIO(response.content)) \n",
        "\n",
        "\n",
        "class_labels = {0: 'Atelectasis',\n",
        "                1: 'Effusion',\n",
        "                2: 'Infiltration',\n",
        "                3: 'No Finding',\n",
        "                4: 'Nodule',\n",
        "                5: 'Pneumothorax'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2qLHzgiJ-ta"
      },
      "source": [
        "# Downloading the data:\n",
        "The following cells will download the pre-processed X-ray images with their accompanying labels.\n",
        "\n",
        "The download (400 MB) may take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0SUE4M0Jmwv"
      },
      "source": [
        "#@title\n",
        "# Downloading the labels of each image:\n",
        "train_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/i6MvQ8nqoiQ9Tci/download')\n",
        "test_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/wLXiOjVAW4AWlXY/download')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFl8GOX_L80K"
      },
      "source": [
        "#@title\n",
        "# Downloading the images:\n",
        "train_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/4rwSf9SYO1ydGtK/download')\n",
        "test_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/dvY2LpvFo6dHef0/download')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation"
      ],
      "metadata": {
        "id": "g4diFISHe8XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods"
      ],
      "metadata": {
        "id": "monXGvmPfE_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(img, label, shift=10, direction='right', roll=True):\n",
        "    \"\"\"\n",
        "    Translates the image in the direction of 'direction' by 'shift' pixels.\n",
        "    \"\"\"\n",
        "    assert direction in ['right', 'left', 'down', 'up'], 'Directions should be top|up|left|right'\n",
        "    img = img.copy()\n",
        "    if direction == 'right':\n",
        "        right_slice = img[:,:, -shift:].copy()\n",
        "        img[:,:, shift:] = img[:,:, :-shift]\n",
        "        if roll:\n",
        "            img[:,:,:shift] = 0\n",
        "    if direction == 'left':\n",
        "        left_slice = img[:,:, :shift].copy()\n",
        "        img[:,:, :-shift] = img[:,:, shift:]\n",
        "        if roll:\n",
        "            img[:,:, -shift:] = 0\n",
        "    if direction == 'down':\n",
        "        down_slice = img[:,-shift:, :].copy()\n",
        "        img[:,shift:, :] = img[:,:-shift,:]\n",
        "        if roll:\n",
        "            img[:,:shift, :] = 0\n",
        "    if direction == 'up':\n",
        "        upper_slice = img[:,:shift, :].copy()\n",
        "        img[:,:-shift, :] = img[:,shift:, :]\n",
        "        if roll:\n",
        "            img[:,-shift:,:] = 0\n",
        "    return img, label\n",
        "\n",
        "def rotate_image(img, label, angle = 15):\n",
        "    \"\"\"\n",
        "    Rotates the images by 'angle' degrees\n",
        "    \"\"\"\n",
        "    img = rotate(input = img, angle = angle, axes = (1,2), reshape = False)\n",
        "    return img, label\n",
        "\n",
        "def gaussian_noise_image(train_image, train_label, noise):\n",
        "    \"\"\"\n",
        "    Adds guassian noise to the input image\n",
        "    \"\"\"\n",
        "    new_image = train_image + noise\n",
        "    return new_image, train_label"
      ],
      "metadata": {
        "id": "hzJL93dbfHvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application"
      ],
      "metadata": {
        "id": "DbQRKlGQf542"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_datasets(train_x, train_y, length =5000 * 6):\n",
        "  \"\"\"\n",
        "  Returns new images and labels arrays with balanced classes.\n",
        "  The images are augmented using the earlier defined functions\n",
        "  such that the classes are balanced.\n",
        "  \"\"\"\n",
        "  augmentations = []\n",
        "\n",
        "  # create two new arrays with the correct length \n",
        "  n = 0\n",
        "  new_train_x, new_train_y = np.empty((length, 1, 128, 128)), np.empty(length)\n",
        "\n",
        "  # extract the initial amount of values per class and how many images are still needed for each class\n",
        "  # create a dictionary that can take into account how many images should be added to the new arrays per class \n",
        "  unique, counts = np.unique(train_y, return_counts=True) \n",
        "  augmentations_per_img = dict(zip(unique, [math.ceil((length/6) / i) for i in counts]))\n",
        "  images_to_go = dict(zip(unique, [length/6] * 6 ))\n",
        "\n",
        "  for i in range(0, train_y.size):\n",
        "    # If the new arrays are not yet filled, and the class is not yet represented as often as it should be\n",
        "    if sum(images_to_go.values()) > 0 and n < length:\n",
        "      if images_to_go[ train_y[i] ] != 0:\n",
        "\n",
        "        # save the current image and its label in the new sets\n",
        "        new_train_x[n], new_train_y[n] = train_x[i], train_y[i]\n",
        "        n += 1\n",
        "        images_to_go[ train_y[i] ] -= 1\n",
        "\n",
        "        # check how often the image should be augmented to reach the required amount of images in the new arrays\n",
        "        augmentations_to_go = augmentations_per_img[ train_y[i]]\n",
        "        methods = [0, 1, 3, 0, 1, 2, 5, 6]\n",
        "\n",
        "        while augmentations_to_go > 0 and images_to_go[ train_y[i] ] > 0 and (n < length):\n",
        "\n",
        "          # randomly assign one of the augmentation methods \n",
        "          augmentations_to_go -= 1\n",
        "          images_to_go[ train_y[i] ] -= 1\n",
        "          value = len(methods)\n",
        "          delete = random.randrange( len(methods) )\n",
        "          method = methods.pop( delete )\n",
        "\n",
        "          if method == 0:\n",
        "            image, label = translate( train_x[i], train_y[i], direction='right', shift=6 )\n",
        "\n",
        "          elif method == 1:\n",
        "            image, label = translate( train_x[i], train_y[i], direction='down', shift=6 )\n",
        "            \n",
        "          elif method == 3:\n",
        "            image, label = translate( train_x[i], train_y[i], direction='left', shift=6 )\n",
        "\n",
        "          elif method == 2:\n",
        "            angle = random.choice([1, 2, 3, 359, 358, 357])  \n",
        "            image, label = rotate_image( train_x[i], train_y[i], angle = angle )\n",
        "\n",
        "          else:\n",
        "            factor = random.choice([250, 230, 210, 190, 170])\n",
        "            # this darkens a few pixels on the image\n",
        "            noise = np.random.binomial(1, 0.99, size = train_x[i].shape) * factor\n",
        "            image, label = gaussian_noise_image( train_x[i], train_y[i], noise )\n",
        "\n",
        "          # add the new image to the array \n",
        "          new_train_x[n] = image\n",
        "          new_train_y[n] = label\n",
        "          n += 1\n",
        "\n",
        "  return new_train_x, new_train_y\n",
        "\n",
        "\n",
        "train_x, train_y = augment_datasets(train_x, train_y)"
      ],
      "metadata": {
        "id": "3Kz_v35Cf8-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building torch dataset"
      ],
      "metadata": {
        "id": "P7UXcGU4kfL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Creates augmented image dataset\"\"\"\n",
        "\n",
        "train_dataset = ImageDataset(train_x, train_y)\n",
        "test_dataset = ImageDataset(test_x, test_y)\n",
        "\n",
        "unique_labels = set(class_labels.keys())"
      ],
      "metadata": {
        "id": "L0x7EPgriZx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNZDXSGYlioO"
      },
      "source": [
        "# Plotting the data distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS0HVYPcMaYO"
      },
      "source": [
        "#@title\n",
        "# Plotting the label distribution in train/test set:\n",
        "fig, ax = plt.subplots(ncols=2, figsize=[20,10])\n",
        "\n",
        "unique, counts = np.unique(train_y, return_counts=True) \n",
        "ax[0].barh([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[0].set_title('Number of images per class in train-set')\n",
        "\n",
        "unique, counts = np.unique(test_y, return_counts=True) \n",
        "ax[1].barh([class_labels[i] + f'\\n({c})' for i, c in zip(unique, counts)], counts)\n",
        "ax[1].set_title('Number of images per class in test-set');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XjlCEzWln29"
      },
      "source": [
        "#Plotting some samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R0kB9rgJ88t"
      },
      "source": [
        "#@title\n",
        "# Plotting some images\n",
        "unique_labels = set(class_labels.keys())\n",
        "fig, ax = plt.subplots(ncols=len(unique_labels), figsize=[25,5])\n",
        "\n",
        "for k, label in enumerate(unique_labels):\n",
        "  ind = list(train_y).index(label)\n",
        "  ax[k].imshow(train_x[ind].reshape(128,128), cmap='gray')\n",
        "  ax[k].set_title(f'Class:{class_labels[train_y[ind]]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPO8ctCicHs"
      },
      "source": [
        "# Defining our model as a neural network:\n",
        "**TODO** define your own model here, follow the structure as presented in the Pytorch tutorial (or see below as an example)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkbIbEU-O-0e"
      },
      "source": [
        "from torch.nn.modules.batchnorm import BatchNorm1d\n",
        "class Net(nn.Module):   \n",
        "    def __init__(self, n_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=4),\n",
        "\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(64, 32, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3),\n",
        "\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(32, 16, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(144, 256),\n",
        "            nn.Linear(256, n_classes),\n",
        "            nn.BatchNorm1d(6),\n",
        "            nn.Softmax(dim=1) \n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        # After our convolutional layers which are 2D, we need to flatten our\n",
        "        # input to be 1 dimensional, as the linear layers require this.\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "# Make sure your model instance is assigned to a variable 'model':\n",
        "model = Net(n_classes = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Yjb3uzm_ar"
      },
      "source": [
        "# Defining our loss and optimizer functions:\n",
        "**TODO** Please define your own optimizer and loss function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the weights of the loss function\n",
        "def calculate_weights(weights_deadly, weights_fact):\n",
        "  \"\"\"\n",
        "  Computes the weights based on \n",
        "  \"\"\"\n",
        "  weights=[i*j for i,j in zip(weights_deadly,weights_fact)]\n",
        "  sum_weights=sum(weights)\n",
        "  weights=[i/sum_weights for i in weights]\n",
        "  return weights\n",
        "\n",
        "class_weights = torch.FloatTensor(calculate_weights([1.5,2,1.5,0.75,1.5,1.25],[4,4,5,5,5,5])).cuda()"
      ],
      "metadata": {
        "id": "X7qTxvy6NxSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0009)\n",
        "loss_function = nn.CrossEntropyLoss(class_weights, [4,4,5,5,5,5])"
      ],
      "metadata": {
        "id": "-4ASQtAOHPBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvkln77gmrOl"
      },
      "source": [
        "#Moving model to CUDA, verifying model structure and printing a summary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5o5m_hEocYY"
      },
      "source": [
        "# IMPORTANT! Set this to True to see actual errors regarding \n",
        "# the structure of your model (CUDA hides them)!\n",
        "# Also make sure you set this to False again for actual model training\n",
        "# as training your model with GPU-acceleration (CUDA) is much faster.\n",
        "DEBUG = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJj3tmq8mnY-"
      },
      "source": [
        "#@title\n",
        "# Moving our model to the right device (CUDA will speed training up significantly!)\n",
        "if torch.cuda.is_available() and not DEBUG:\n",
        "  device = 'cuda'\n",
        "  model.to(device)\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)\n",
        "else:\n",
        "  device='cpu'\n",
        "  # Creating a summary of our model and its layers:\n",
        "  summary(model, (1, 128, 128), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1Np1S3nNoh"
      },
      "source": [
        "#Defining our training/testing and accuracy methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz7CfpfYnM_b"
      },
      "source": [
        "#@title\n",
        "def train_model(model, train_sampler, optimizer, loss_function):\n",
        "  # Lets keep track of all the losses:\n",
        "  losses = []\n",
        "  # Put the model in train mode:\n",
        "  model.train()\n",
        "  # Feed all the batches one by one:\n",
        "  for batch in train_sampler:\n",
        "    # Get a batch:\n",
        "    x, y = batch\n",
        "    # Making sure our samples are stored on the same device as our model: \n",
        "    x, y = x.to(device), y.to(device)\n",
        "    # Get predictions:\n",
        "    predictions = model.forward(x)\n",
        "    loss = loss_function(predictions,y)\n",
        "    losses.append(loss)\n",
        "    # We first need to make sure we reset our optimizer at the start.\n",
        "    # We want to learn from each batch seperately, \n",
        "    # not from the entire dataset at once.\n",
        "    optimizer.zero_grad()\n",
        "    # We now backpropagate our loss through our model:\n",
        "    loss.backward()\n",
        "    # We then make the optimizer take a step in the right direction.\n",
        "    optimizer.step()\n",
        "  return losses\n",
        "\n",
        "def test_model(model, test_sampler, loss_function):\n",
        "  # Setting the model to evaluation mode:\n",
        "  model.eval()\n",
        "  losses = []\n",
        "  # We need to make sure we do not update our model based on the test data:\n",
        "  with torch.no_grad():\n",
        "    for (x, y) in test_sampler:\n",
        "      # Making sure our samples are stored on the same device as our model:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      prediction = model.forward(x)\n",
        "      loss = loss_function(prediction,y)\n",
        "      losses.append(loss)\n",
        "  return losses\n",
        "\n",
        "def calculate_accuracy(model):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    for (x, y) in test_sampler:\n",
        "      # Making sure our samples are stored on the same device as our model:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      prediction = model.forward(x).argmax(axis=1)\n",
        "      correct += sum(prediction == y)\n",
        "      count += len(y)\n",
        "  accuracy = (correct/count).detach().cpu().numpy()\n",
        "  return float(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDG8XNh_ojrW"
      },
      "source": [
        "#Training our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5mPvRt5op0z"
      },
      "source": [
        "n_epochs = 50\n",
        "batch_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAunFnrpSDKU"
      },
      "source": [
        "\n",
        "\n",
        "#@title\n",
        "# Lets now train and test our model for multiple epochs:\n",
        "train_sampler = BatchSampler(batch_size=batch_size, dataset=train_dataset, balanced=False)\n",
        "test_sampler = BatchSampler(batch_size=100, dataset=test_dataset, balanced=False)\n",
        "\n",
        "mean_losses_train = []\n",
        "mean_losses_test = []\n",
        "accuracies = []\n",
        "for e in range(n_epochs):\n",
        "  # Training\n",
        "  losses = train_model(model, train_sampler, optimizer, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_train.append(mean_loss)\n",
        "  print(f'\\nEpoch {e + 1} training done, loss on train set: {torch.mean(mean_loss)}\\n')\n",
        "\n",
        "  # Testing:\n",
        "  losses = test_model(model, test_sampler, loss_function)\n",
        "  # Calculating and printing statistics:\n",
        "  mean_loss = sum(losses) / len(losses)\n",
        "  mean_losses_test.append(mean_loss)\n",
        "  accuracy = calculate_accuracy(model)\n",
        "  print(accuracy)\n",
        "  print(f'\\nEpoch {e + 1} testing done, loss on test set: {mean_loss}\\n')\n",
        "\n",
        "  # Saving the model with the highest accuracy\n",
        "  if (e == 0):\n",
        "    # lowest_loss = mean_loss\n",
        "    highest_acc = accuracy\n",
        "    best_model = copy.deepcopy(model)\n",
        "    # weights = list(model.parameters())\n",
        "  if (highest_acc < accuracy):\n",
        "    highest_acc = accuracy\n",
        "    best_model = copy.deepcopy(model)\n",
        "  # Plotting the historic loss:\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(mean_losses_train, label='Train loss')\n",
        "  ax.plot(mean_losses_test, label='Test loss')\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "  \n",
        "model = best_model\n",
        "\n",
        "print(calculate_accuracy(best_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tnZQoGJqKND"
      },
      "source": [
        "#Evaluation our model:\n",
        "**TODO** write your own methods to evaluate the model. For example, calculate the accuracy of the model on the test-set:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "BUlTCaNuILfB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw8ox5pYqWaD"
      },
      "source": [
        "calculate_accuracy(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3nM958rnYPH"
      },
      "source": [
        "This accuracy isn't great. Your task is to find a better model that performs better at the classification task. Other methods of evaluation might tell you more why a particular model is not performing well (accuracy is a quite limited aggregated performance metric). "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saliency map"
      ],
      "metadata": {
        "id": "gmsIfji9ws9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_saliency_scores(model, train_x, train_y, i=0):\n",
        "  \"\"\"\"\n",
        "  Returns the saliency of an image and also the corresponding\n",
        "  original image and the label\n",
        "  \"\"\"\n",
        "  \n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  image = train_x[i]                # get the image to map, this must be a np.ndarray\n",
        "  img = image                       # safe to use as the reference picture of the scan\n",
        "  image.resize([1, 1, 128, 128])\n",
        "  image = torch.from_numpy(image).float()\n",
        "\n",
        "  image = image.to(device)\n",
        "  image.requires_grad_()            # catch the gradient during the backpropagation\n",
        "\n",
        "  output = model.forward(image)     # run the picture through the model\n",
        "  output_idx = output.argmax()\n",
        "  output_max = output[0, output_idx]\n",
        "  output_max.backward()\n",
        "\n",
        "  prediction = class_labels[ (torch.argmax(output, 1)[0]).item() ]\n",
        "  saliency, _ = torch.max(image.grad.data.abs(), dim=1)\n",
        "  saliency = saliency.reshape(128, 128)\n",
        "  return saliency, img, prediction\n",
        "\n",
        "def draw_saliency_map(model, train_x, train_y, i=0):\n",
        "  saliency_map = 0\n",
        "  index = i\n",
        "  #Find a picture with a saliency map that is not black\n",
        "  while ( saliency_map == 0 ):  \n",
        "    index += 1\n",
        "    saliency, img, prediction = get_saliency_scores(model, train_x, train_y, index)\n",
        "    saliency_map = torch.sum(saliency)\n",
        "    img = img.reshape(-1, 128, 128)\n",
        "    \n",
        "  #Draw the image and its saliency map\n",
        "\n",
        "  fig, ax = plt.subplots(1, 2)\n",
        "  ax[0].imshow(img.reshape(128, 128), cmap='gray')\n",
        "  ax[0].axis('off')\n",
        "  ax[1].imshow(saliency.cpu(), cmap='hot')\n",
        "  ax[1].axis('off')\n",
        "  plt.tight_layout()\n",
        "  fig.suptitle(f'Image and Its Saliency Map. Class: $\\it{class_labels[train_y[i]]}$, Prediction: $\\it{prediction}$')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "R1T0nMPBwwFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_saliency_map(model, train_x, train_y, i=0)"
      ],
      "metadata": {
        "id": "-V0_iNKhycWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Certainty score"
      ],
      "metadata": {
        "id": "1uJWlcYczUxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_tensor(image_index, dataset):\n",
        "    \"\"\"\n",
        "    Returns the image as a tensor\n",
        "    \"\"\"\n",
        "    image = [dataset[image_index][0]]\n",
        "    # label optional, just to keep track\n",
        "    label = [dataset[image_index][1]]\n",
        "    # for final use, delete the 2nd object - the label (not present for \"real\" test image)\n",
        "    return (torch.stack(image).float(), torch.tensor(label).long())\n",
        "\n",
        "def prediction_scores(model, sample):\n",
        "  \"\"\"\n",
        "  Predicts the label of a given input image\n",
        "  \"\"\"\n",
        "  # Setting the model to evaluation mode:\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    x, y = sample\n",
        "    x = x.to(device)\n",
        "    prediction = model.forward(x)\n",
        "    #The ouput is normalized so it can be interpreted as probabilities\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "R9YydHzEzZAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDWaRA3xZeB2"
      },
      "source": [
        "## Confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvisewabIt33"
      },
      "source": [
        "@torch.no_grad()\n",
        "def get_Y_pred(model, test_set):\n",
        "  \"\"\"Returns the predictions made the array 'test_set'\"\"\"\t\n",
        "  # we want this functions execution to omit gradient tracking\n",
        "  model.eval()\n",
        "  y_preds = torch.tensor([]).cuda()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    loader = DataLoader(test_set, batch_size = 500)\n",
        "\n",
        "  for batch in loader:\n",
        "      images, labels = batch\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      preds = model(images)\n",
        "      y_preds = torch.cat( (y_preds, preds) , dim=0)\n",
        "\n",
        "  return y_preds.argmax(dim=1).cpu()\n",
        "    \n",
        "def get_matrix():\n",
        "  \"\"\"\n",
        "  Return the confusion matrix\n",
        "  \"\"\"\n",
        "  y_preds = get_Y_pred(model, test_dataset)\n",
        "  matrix = confusion_matrix(test_dataset.targets, y_preds)\n",
        "  return matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_matrix()"
      ],
      "metadata": {
        "id": "BoFRjXiT1OQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_SjPgLUZhM5"
      },
      "source": [
        "## Recall, Precision, F1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIpstPN0jtJQ"
      },
      "source": [
        "def one_hot_encode(labels):\n",
        "  \"\"\"\n",
        "  Creates a list of lists with the labels one hot encoded\n",
        "  for every label\n",
        "  \"\"\"\n",
        "  encoded_labels = []\n",
        "  for k in range(len(labels)):\n",
        "    encoded_labels.append([1 if labels[k] == i else 0 for i in range(6)])\n",
        "  return torch.tensor(encoded_labels).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jLdAQQhdmef"
      },
      "source": [
        "\n",
        "def calculate_stats():\n",
        "  \"\"\"\n",
        "  Returns a dataframe contain the recall, precision,\n",
        "  F1 and ROC AUC scores\n",
        "  \"\"\"\n",
        "  y_preds = get_Y_pred(model, test_dataset)\n",
        "  y_true = test_dataset.targets\n",
        "\n",
        "  Precision = precision_score(y_true, y_preds, average=None)\n",
        "  Recall = recall_score(y_true, y_preds, average=None)\n",
        "  F1 = f1_score(y_true, y_preds, average=None)\n",
        "\n",
        "  y_true_encoded = one_hot_encode(y_true)\n",
        "  y_preds_encoded = one_hot_encode(y_preds)\n",
        "  roc_auc = roc_auc_score(y_true_encoded.cpu(), y_preds_encoded.cpu(), multi_class='ovr', average=None) # one vs rest\n",
        "\n",
        "  scores = pd.DataFrame( { \"Recall\" : Recall, \"Precision\" : Precision , \"F1\" : F1, 'roc auc' : roc_auc}, \n",
        "                        index=[\"Atelectasis\", 'Effusion', 'Infiltration', 'No Finding', 'Nodule', 'Pneumothorax'])\n",
        "\n",
        "  # https://vitalflux.com/micro-average-macro-average-scoring-metrics-multi-class-classification-python/\n",
        "  scores.loc['mean (macro)', 'Recall'] =    recall_score(y_true, y_preds, average='macro')\n",
        "  scores.loc['mean (macro)', 'Precision'] = precision_score(y_true, y_preds, average='macro')\n",
        "  scores.loc['mean (macro)', 'F1'] =        f1_score(y_true, y_preds, average='macro')\n",
        "  scores.loc['mean (macro)', 'roc auc'] =   roc_auc_score(y_true_encoded.cpu(), y_preds_encoded.cpu(), multi_class='ovr', average='macro')\n",
        "\n",
        "  return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_stats()"
      ],
      "metadata": {
        "id": "71KQqYxk55jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TKxbIT1mPoX"
      },
      "source": [
        "\"\"\"\n",
        "Plots the receiver operating characteristic\n",
        "\"\"\"\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "y_true = test_dataset.targets\n",
        "y_preds = get_Y_pred(model, test_dataset)\n",
        "y_true_encoded = one_hot_encode(y_true)\n",
        "y_preds_encoded = one_hot_encode(y_preds) \n",
        "\n",
        "for i in range(6):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_encoded[:, i].cpu(), y_preds_encoded[:, i].cpu())\n",
        "    roc_auc[i] = np.round( auc(fpr[i], tpr[i]), 3)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr[0], tpr[0], color='blue', lw=2, label= f'Atelect. (area = {roc_auc[0]})')\n",
        "plt.plot(fpr[1], tpr[1], color='green', lw=2, label= f'Effusion. (area = {roc_auc[1]})')\n",
        "plt.plot(fpr[2], tpr[2], color=\"darkorange\", lw=2, label= f\"Inftilt. (area = {roc_auc[2]})\")\n",
        "plt.plot(fpr[3], tpr[3], color='brown', lw=2, label= f'Nothing. (area = {roc_auc[3]})')\n",
        "plt.plot(fpr[4], tpr[4], color='pink', lw=2, label= f'Nodule. (area = {roc_auc[4]})')\n",
        "plt.plot(fpr[5], tpr[5], color='red', lw=2, label= f'Pneum. (area = {roc_auc[5]})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROX) curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running all evaluations"
      ],
      "metadata": {
        "id": "YdB_a2AiKldH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  \"\"\"\"\n",
        "  Performes all the evaluation methods on the model\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "\n",
        "  accuracy = calculate_accuracy()\n",
        "  y_preds = get_Y_pred(model, test_dataset)\n",
        "  matrix = get_matrix()\n",
        "  scores = calculate_stats()\n",
        "\n",
        "  return accuracy, y_preds, matrix, scores"
      ],
      "metadata": {
        "id": "mtMbmHSUKn8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the evaluations"
      ],
      "metadata": {
        "id": "FaoKICy76nJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_df(models_df, batch_size):\n",
        "  \"\"\"\n",
        "  Adds all the performance metrics to the input dataframe\n",
        "  \"\"\"\n",
        "\n",
        "  # calculate the performance metrics\n",
        "  accuracy, y_preds, matrix, scores = evaluate()\n",
        "\n",
        "  models_df.loc[run, 'n epochs'] = e\n",
        "  models_df.loc[run, 'batch size'] = batch_size\n",
        "\n",
        "  models_df.loc[run, 'accuracy'] = accuracy\n",
        "\n",
        "  # Add the class with the least amount of predictions and the correspondig number of predictions\n",
        "  avg = np.sum( matrix, axis=0 )\n",
        "  models_df.loc[run, 'min predictions nr'] = avg.min()\n",
        "  models_df.loc[run, 'min predictions group'] = str( [class_labels[i] for i in np.where( avg == avg.min() )[0]] )\n",
        "  models_df.loc[run, 'matrix'] = str( matrix )\n",
        "\n",
        "  # add the performance metrics to the dataframe\n",
        "  for index, row in scores.iterrows():\n",
        "    if index != 'mean (macro)':\n",
        "      models_df.loc[run, [f'{index} Recall', f'{index} Precision', f'{index} F1', f'{index} roc'] ] = row['Recall'], row['Precision'], row['F1'], row['roc auc']\n",
        "    else:\n",
        "      models_df.loc[run, [f'{index} Recall etc', f'{index} roc'] ] = row['Recall'], row['roc auc']"
      ],
      "metadata": {
        "id": "1Wpu_sG56qif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter tuning"
      ],
      "metadata": {
        "id": "MEjfmZuM7Yjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is used to tune the hyperparameters. This takes a couple of hours to run therefore it is commented out. The correct parameters are already implemented in the code above. To run the code, just remove the # signs."
      ],
      "metadata": {
        "id": "nx0HmPhy7cm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This code is used to get all the combinations possible of batches,\n",
        "learning rates and decay factors.\n",
        "\"\"\"\n",
        "\n",
        "# batches =       [8, 16, 32, 64, 100, 128, 200, 256]\n",
        "# learningRates = [0.00001, 0.00005, 0.0001, 0.0003, 0.0006, 0.0009, \n",
        "#                  0.001, 0.003, 0.006, 0.009, 0.01, 0.03, 0.06, 0.09]\n",
        "# decays =        [True, False]\n",
        "\n",
        "# combinations = []\n",
        "\n",
        "# # get all combinations of the three hyperparameters\n",
        "# for lr in learningRates:\n",
        "#   for batch in batches:\n",
        "#     for decay in decays:\n",
        "#       combination = [batch, lr, decay]\n",
        "#       combinations.append(combination)\n",
        "\n",
        "# models_df = pd.DataFrame()\n",
        "# run = 0"
      ],
      "metadata": {
        "id": "HRH6QCDr73TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Runs all the possible combinations of parameters and saves the results to a dataframe.\n",
        "\"\"\"\n",
        "\n",
        "# device = 'cuda' if ( torch.cuda.is_available and not DEBUG) else 'cpu'\n",
        "# n_epochs = 13\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# ############################################################################################\n",
        "# for parameters in combinations:\n",
        "\n",
        "#   model = Net(n_classes=6)  # create a model\n",
        "#   model.to(device)\n",
        "\n",
        "#   batch_size = parameters[0]\n",
        "\n",
        "#   train_sampler = BatchSampler(batch_size = batch_size, dataset=train_dataset, balanced=False)\n",
        "#   test_sampler = BatchSampler(batch_size = 100, dataset=test_dataset, balanced=False)\n",
        "\n",
        "#   optimizer = optim.Adam(model.parameters(), lr = parameters[1])\n",
        "# ############################################################################################  \n",
        "#   mean_losses_train = []\n",
        "#   mean_losses_test = []\n",
        "#   accuracies = []\n",
        "\n",
        "#   print(f'\\n batch size {parameters[0]}, learning rate {parameters[1]}, and decay is {parameters[2]}')\n",
        "\n",
        "#   for e in range(n_epochs):\n",
        "#     # if weight decay is used, change the value in the optimizer each epoch\n",
        "#     if parameters[2] and e > 0:\n",
        "#       decay = parameters[1] / math.sqrt(e)\n",
        "#       optimizer = optim.Adam(model.parameters(), lr = parameters[1], weight_decay = decay)\n",
        "\n",
        "#     # Training:\n",
        "#     losses = train_model(model, train_sampler, optimizer, loss_function)\n",
        "#     mean_loss = sum(losses) / len(losses)\n",
        "#     mean_losses_train.append(mean_loss)\n",
        "  \n",
        "#     # Testing:\n",
        "#     losses = test_model(model, test_sampler, loss_function)\n",
        "#     mean_loss = sum(losses) / len(losses)\n",
        "#     mean_losses_test.append(mean_loss)\n",
        "#     # Plotting the historic loss:\n",
        "    \n",
        "#   fig, ax = plt.subplots()\n",
        "#   ax.plot(mean_losses_train, label='Train loss')\n",
        "#   ax.plot(mean_losses_test, label='Test loss')\n",
        "#   ax.legend()\n",
        "#   plt.show()\n",
        "\n",
        "#   add_to_df(models_df, batch_size)\n",
        "#   models_df.loc[run, 'learning rate'] = parameters[1]\n",
        "#   models_df.loc[run, 'weight decay'] = parameters[2]\n",
        "#   run += 1\n",
        "\n",
        "#   # save the data in a hyperparameter tuning csv file\n",
        "#   models_df.to_csv('hyperparameter tuning.csv')"
      ],
      "metadata": {
        "id": "-efnUQvp7_sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations of the hyperparameter tuning"
      ],
      "metadata": {
        "id": "0mXIsbky9L2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Learning rate"
      ],
      "metadata": {
        "id": "WkeLKmOW-Aj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Makes barplots containing the accuracy of the model with different \n",
        "learning rates\n",
        "\"\"\"\n",
        "# data = pd.read_csv('hyperparameter tuning.csv')\n",
        "\n",
        "# fig, ax = plt.subplots(nrows = 2, ncols = 2, squeeze=True, sharex=True)\n",
        "# sns.set(rc={\"figure.figsize\":(25, 10)} , style=\"whitegrid\")\n",
        "# data['batch size'] = [int(i) for i in data['batch size']]\n",
        "\n",
        "# sns.barplot(y = data['accuracy'], x=data['learning rate'] , ax=ax[0, 0] )\n",
        "# sns.barplot(x = data['learning rate'], y=data['Atelectasis F1'] , ax=ax[0, 1])\n",
        "# sns.barplot(x = data['learning rate'], y=data['Effusion F1'] , ax=ax[1, 0])\n",
        "# sns.barplot(x = data['learning rate'], y=data['Infiltration F1'] , ax=ax[1, 1]);\n",
        "# ax[1,0].set_xticklabels(labels = ax[1,0].get_xticklabels(), rotation=45);\n",
        "# ax[1,1].set_xticklabels(labels = ax[1,1].get_xticklabels(), rotation=45);"
      ],
      "metadata": {
        "id": "mMwoguin9P6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch size"
      ],
      "metadata": {
        "id": "6np4HCga-HMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates barplots containing the accuracy of different batch sizes\n",
        "\"\"\"\n",
        "\n",
        "# data = pd.read_csv('hyperparameter tuning.csv')\n",
        "\n",
        "# fig, ax = plt.subplots(nrows = 2, ncols = 2, squeeze=True, sharex=True)\n",
        "\n",
        "# sns.set(rc={\"figure.figsize\":(25, 10)} , style=\"whitegrid\")\n",
        "# data['batch size'] = [int(i) for i in data['batch size']]\n",
        "\n",
        "# sns.barplot(y = data['accuracy'], x=data['batch size'] , ax=ax[0, 0] )\n",
        "# sns.barplot(x = data['batch size'], y=data['Atelectasis F1'] , ax=ax[0, 1])\n",
        "# sns.barplot(x = data['batch size'], y=data['Effusion F1'] , ax=ax[1, 0])\n",
        "# sns.barplot(x = data['batch size'], y=data['Infiltration F1'] , ax=ax[1, 1]);\n",
        "# ax[1,0].set_xticklabels(labels = ax[1,0].get_xticklabels(), rotation=45);\n",
        "# ax[1,1].set_xticklabels(labels = ax[1,1].get_xticklabels(), rotation=45);"
      ],
      "metadata": {
        "id": "hyXxOkio-S-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight decay"
      ],
      "metadata": {
        "id": "Ye3KslyZ-iW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates barplots containing the accuracy of different decay factors\n",
        "\"\"\"\n",
        "\n",
        "# data = pd.read_csv('hyperparameter tuning.csv')\n",
        "\n",
        "# fig, ax = plt.subplots(nrows = 2, ncols = 2, squeeze=True, sharex=True)\n",
        "\n",
        "# sns.set(rc={\"figure.figsize\":(25, 10)} , style=\"whitegrid\")\n",
        "\n",
        "# sns.barplot(y = data['accuracy'], x=data['weight decay'] , ax=ax[0, 0] )\n",
        "# sns.barplot(x = data['weight decay'], y=data['Atelectasis F1'] , ax=ax[0, 1])\n",
        "# sns.barplot(x = data['weight decay'], y=data['Effusion F1'] , ax=ax[1, 0])\n",
        "# sns.barplot(x = data['weight decay'], y=data['Infiltration F1'] , ax=ax[1, 1]);\n",
        "# ax[1,0].set_xticklabels(labels = ax[1,0].get_xticklabels(), rotation=45);\n",
        "# ax[1,1].set_xticklabels(labels = ax[1,1].get_xticklabels(), rotation=45);"
      ],
      "metadata": {
        "id": "M-THqx2p-oOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the final model 5 times"
      ],
      "metadata": {
        "id": "bMl1YDqJ_QX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model using the hyperparameters found is trained and evaluated 5 times. From this we obtained the average accuracy of 28.73%. To run this section, just comment out everything. It is important to note that this code only works when you run the hyperparameter tune before this code."
      ],
      "metadata": {
        "id": "QVENX38n_hrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell trains the model 5 times using the earlier found hyperparameters\n",
        "\"\"\"\n",
        "# n_epochs = 50 #change value to 1 for debugging reasons\n",
        "# batch_size = 32\n",
        "\n",
        "# DEBUG = False\n",
        "# device = 'cuda' if ( torch.cuda.is_available and not DEBUG) else 'cpu'\n",
        "# model.to(device)\n",
        "\n",
        "# unique_labels = set(class_labels.keys())\n",
        "\n",
        "\n",
        "# performanceScores = np.array()\n",
        "# accuracies = np.array()\n",
        "# matrices = np.array()\n",
        "\n",
        "\n",
        "# for (i in range(0, 5)):\n",
        "#   # download and augment the data\n",
        "#   train_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/i6MvQ8nqoiQ9Tci/download')\n",
        "#   test_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/wLXiOjVAW4AWlXY/download')\n",
        "\n",
        "#   train_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/4rwSf9SYO1ydGtK/download')\n",
        "#   test_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/dvY2LpvFo6dHef0/download')\n",
        "\n",
        "#   train_x, train_y = augment_datasets(train_x, train_y)\n",
        "\n",
        "#   train_dataset = ImageDataset(train_x, train_y)\n",
        "#   test_dataset = ImageDataset(test_x, test_y)\n",
        "\n",
        "\n",
        "#   # initiate a new model, the loss function and the optimizer\n",
        "#   model = Net(n_classes=6)  \n",
        "  \n",
        "#   train_sampler = BatchSampler(batch_size = batch_size, dataset=train_dataset, balanced=False)\n",
        "#   test_sampler = BatchSampler(batch_size = 100, dataset=test_dataset, balanced=False)\n",
        "\n",
        "#   optimizer = optim.Adam(model.parameters(), lr=0.0009)\n",
        "#   loss_function = nn.CrossEntropyLoss( torch.Tensor( [0.1538, 0.2051, 0.1923, 0.0962, 0.1923, 0.1603] ).to( device ) )\n",
        "\n",
        "\n",
        "#   # go through the training and testing process for each combination\n",
        "#   mean_losses_train = []\n",
        "#   mean_losses_test = []\n",
        "#   accuracies = []\n",
        "\n",
        "#   for e in range(n_epochs):\n",
        "\n",
        "#     if parameters[2] and e > 0:\n",
        "#       decay = parameters[1] / math.sqrt(e) # https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "#       optimizer = optim.Adam(model.parameters(), lr = parameters[1], weight_decay = decay)\n",
        "\n",
        "#     # Training:\n",
        "#     losses = train_model(model, train_sampler, optimizer, loss_function)\n",
        "#     mean_loss = sum(losses) / len(losses)\n",
        "#     mean_losses_train.append(mean_loss)\n",
        "  \n",
        "#     # Testing:\n",
        "#     losses = test_model(model, test_sampler, loss_function)\n",
        "#     mean_loss = sum(losses) / len(losses)\n",
        "#     mean_losses_test.append(mean_loss)\n",
        "#     # Plotting the historic loss:\n",
        "    \n",
        "\n",
        "#   # extract the scores and append to the numpy arrays\n",
        "#   scores = calculate_stats()\n",
        "\n",
        "#   np.append( performanceScores, scores.to_numpy() )\n",
        "#   np.append( accuracies, calculate_accuracy() )\n",
        "#   np.append( matrices, get_matrix() ) \n",
        "\n"
      ],
      "metadata": {
        "id": "_uEtoKyn_0x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a heatmap containing the average confusion matrix of the 5 training\n",
        "attempts.\n",
        "\"\"\"\n",
        "\n",
        "# meanMatrix = np.mean( matrices, axis=0 )\n",
        "\n",
        "# #draw the resulting matrix\n",
        "\n",
        "# labels = ['Atelectasis', 'Effusion',  'Infiltration', 'No Finding', 'Nodule', 'Pneumothorax']\n",
        "# sns.heatmap(meanMatrix,annot=True,cmap='Blues', fmt='g', xticklabels= labels , yticklabels= labels, xlabel)"
      ],
      "metadata": {
        "id": "Oj--ns4yA_NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a dataframe with the mean values for the five runs.\n",
        "\"\"\"\n",
        "\n",
        "# meanScores = np.mean( performanceScores, axis=0 )\n",
        "# pd.DataFrame(np.round( meanScores, 4), \n",
        "#              index=[\"Atelectasis\", 'Effusion', 'Infiltration', 'No Finding', 'Nodule', 'Pneumothorax', 'mean (macro)'], \n",
        "#              columns= [\t'Recall',\t'Precision',\t'F1', \t'roc auc'])"
      ],
      "metadata": {
        "id": "PLSDbqCnBftY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculates the mean accuracy\n",
        "\"\"\"\n",
        "\n",
        "# meanAcc = np.mean( accuracies, axis=0 )\n",
        "# print(meanAcc)"
      ],
      "metadata": {
        "id": "x9pySgRfB6rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uncertainty scores"
      ],
      "metadata": {
        "id": "tjCs5Ht3P2Jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is used to get the certainty score for a given input image. It returns an output string which represents the certainty of the perdicted label."
      ],
      "metadata": {
        "id": "3x_jyjKgQfuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to suppress scientific notation (exponential)\n",
        "torch.set_printoptions(sci_mode=False)"
      ],
      "metadata": {
        "id": "YKPiVOt2P5eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_certainty(model, sample):\n",
        "  \"\"\"\n",
        "  Gives a certainty of a prediction represented by a string.\n",
        "  \"\"\"\n",
        "  scores = prediction_scores(model, sample)\n",
        "  for i in scores.cpu().numpy()[0]:\n",
        "      if i > 0.95:\n",
        "          return \"High certainty\"\n",
        "  for i in scores.cpu().numpy()[0]:\n",
        "      if i > 0.8:\n",
        "          return \"Medium certainty\"\n",
        "  return \"Low certainty\"\n",
        "\n",
        "def predicted_label(model, sample):\n",
        "  \"\"\"\n",
        "  Returns the predicted label of a given input\n",
        "  \"\"\"\n",
        "  scores = prediction_scores(model, sample)[0]\n",
        "  max_score = 0\n",
        "  max_i = 0\n",
        "  for i in range(0, len(scores)):\n",
        "      if scores[i]>max_score:\n",
        "          max_score = scores[i]\n",
        "          max_i = i\n",
        "  return max_i\n",
        "\n",
        "def evaluate_certainty(model, ind_min, ind_max, dataset):\n",
        "    \"\"\"\n",
        "    Returns a table containing all the number of occurances of the three\n",
        "    different certainty levels\n",
        "    \"\"\"\n",
        "    matrix = pd.DataFrame()\n",
        "    matrix['certainty'] = ['High certainty', 'Medium certainty', 'Low certainty']\n",
        "    matrix['correct prediction'] = [0,0,0]\n",
        "    matrix['wrong prediction'] = [0,0,0]\n",
        "    # matrix = matrix.set_index('certainty')\n",
        "    \n",
        "    for i in range(ind_min, ind_max+1):\n",
        "        sample = image_to_tensor(i, dataset)\n",
        "        true_label = sample[1]\n",
        "        pred_label = predicted_label(model, sample)\n",
        "        output_cert = output_certainty(model, sample)\n",
        "        cert_dict = {'High certainty':0, 'Medium certainty':1, 'Low certainty':2}\n",
        "        output_cert_ind = cert_dict[output_cert]\n",
        "        if true_label == pred_label:\n",
        "            matrix['correct prediction'][output_cert_ind] +=1\n",
        "        else:\n",
        "            matrix['wrong prediction'][output_cert_ind] +=1\n",
        "            \n",
        "    return matrix"
      ],
      "metadata": {
        "id": "2AhIaPvVQS9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncertainty_table = evaluate_certainty(model, 0, len(test_dataset)-1, test_dataset)\n",
        "uncertainty_table['sum'] = uncertainty_table.sum(axis=1)\n",
        "uncertainty_table['correct prediction %'] = uncertainty_table['correct prediction']/uncertainty_table['sum']*100\n",
        "uncertainty_table"
      ],
      "metadata": {
        "id": "GN5UB5W0RZ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1fo7K_1mq1L"
      },
      "source": [
        "#Saving our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esVBn4-N1ij9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/weights_model.txt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}